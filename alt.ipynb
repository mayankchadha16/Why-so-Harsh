{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let us start by importing the library from the package directory\n",
    "import pandas as pd\n",
    "\n",
    "# Read the data\n",
    "df = pd.read_csv(\"./train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>harsh</th>\n",
       "      <th>extremely_harsh</th>\n",
       "      <th>vulgar</th>\n",
       "      <th>threatening</th>\n",
       "      <th>disrespect</th>\n",
       "      <th>targeted_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a8be7c5d4527adbbf15f</td>\n",
       "      <td>\", 6 December 2007 (UTC)\\nI am interested, not...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0b7ca73f388222aad64d</td>\n",
       "      <td>I added about three missing parameters to temp...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>db934381501872ba6f38</td>\n",
       "      <td>SANDBOX?? \\n\\nI DID YOUR MADRE DID IN THE SANDBOX</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>228015c4a87c4b1f09a7</td>\n",
       "      <td>why good sir? Why? \\n\\nYou, sir, obviously do ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b18f26cfa1408b52e949</td>\n",
       "      <td>\"\\n\\n Source \\n\\nIncase I forget, or someone e...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "0  a8be7c5d4527adbbf15f  \", 6 December 2007 (UTC)\\nI am interested, not...   \n",
       "1  0b7ca73f388222aad64d  I added about three missing parameters to temp...   \n",
       "2  db934381501872ba6f38  SANDBOX?? \\n\\nI DID YOUR MADRE DID IN THE SANDBOX   \n",
       "3  228015c4a87c4b1f09a7  why good sir? Why? \\n\\nYou, sir, obviously do ...   \n",
       "4  b18f26cfa1408b52e949  \"\\n\\n Source \\n\\nIncase I forget, or someone e...   \n",
       "\n",
       "   harsh  extremely_harsh  vulgar  threatening  disrespect  targeted_hate  \n",
       "0      0                0       0            0           0              0  \n",
       "1      0                0       0            0           0              0  \n",
       "2      1                0       0            0           0              0  \n",
       "3      1                0       1            1           1              0  \n",
       "4      0                0       0            0           0              0  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the first five rows of our dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_url(review_text):\n",
    "    return re.sub(r'http\\S+', ' ', review_text)\n",
    "\n",
    "df['text'] = df['text'].apply(clean_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove html tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html_tags(review_text):\n",
    "    return re.sub('<[^<]+?>', '', review_text)\n",
    "\n",
    "df['text'] = df['text'].apply(clean_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove numbers and punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_non_alphanumeric(review_text):\n",
    "    return re.sub('[^a-zA-Z]', ' ', review_text)\n",
    "\n",
    "df['text'] = df['text'].apply(clean_non_alphanumeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     December       UTC  I am interested  not in arguing  but in the policies which resolve our ongoing content dispute  Also  see Wikipedia  WikiProject United States presidential elections for what I ll be working on  Also  the moneybomb closer just self reverted on two different requests  which echoed what I would have requested   I will rephrase     which I didn t see an answer to  building on our agreement that   moneybomb   should not be a redlink  Given the deletion reversion  what should be the outline of the article called   moneybomb   or should it be submitted for AFD again in due time   If the latter  see the previous version of      However  this version will require a detailed answer because any ambiguity will only necessitate clarifying questions          '"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = df['text'].apply(lambda x: x.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'     december       utc  i am interested  not in arguing  but in the policies which resolve our ongoing content dispute  also  see wikipedia  wikiproject united states presidential elections for what i ll be working on  also  the moneybomb closer just self reverted on two different requests  which echoed what i would have requested   i will rephrase     which i didn t see an answer to  building on our agreement that   moneybomb   should not be a redlink  given the deletion reversion  what should be the outline of the article called   moneybomb   or should it be submitted for afd again in due time   if the latter  see the previous version of      however  this version will require a detailed answer because any ambiguity will only necessitate clarifying questions          '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spell Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from textblob import TextBlob\n",
    "\n",
    "# df['text'] = df['text'].apply(lambda x : TextBlob(x).correct())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "df['text'] = df['text'].apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['december',\n",
       " 'utc',\n",
       " 'i',\n",
       " 'am',\n",
       " 'interested',\n",
       " 'not',\n",
       " 'in',\n",
       " 'arguing',\n",
       " 'but',\n",
       " 'in',\n",
       " 'the',\n",
       " 'policies',\n",
       " 'which',\n",
       " 'resolve',\n",
       " 'our',\n",
       " 'ongoing',\n",
       " 'content',\n",
       " 'dispute',\n",
       " 'also',\n",
       " 'see',\n",
       " 'wikipedia',\n",
       " 'wikiproject',\n",
       " 'united',\n",
       " 'states',\n",
       " 'presidential',\n",
       " 'elections',\n",
       " 'for',\n",
       " 'what',\n",
       " 'i',\n",
       " 'll',\n",
       " 'be',\n",
       " 'working',\n",
       " 'on',\n",
       " 'also',\n",
       " 'the',\n",
       " 'moneybomb',\n",
       " 'closer',\n",
       " 'just',\n",
       " 'self',\n",
       " 'reverted',\n",
       " 'on',\n",
       " 'two',\n",
       " 'different',\n",
       " 'requests',\n",
       " 'which',\n",
       " 'echoed',\n",
       " 'what',\n",
       " 'i',\n",
       " 'would',\n",
       " 'have',\n",
       " 'requested',\n",
       " 'i',\n",
       " 'will',\n",
       " 'rephrase',\n",
       " 'which',\n",
       " 'i',\n",
       " 'didn',\n",
       " 't',\n",
       " 'see',\n",
       " 'an',\n",
       " 'answer',\n",
       " 'to',\n",
       " 'building',\n",
       " 'on',\n",
       " 'our',\n",
       " 'agreement',\n",
       " 'that',\n",
       " 'moneybomb',\n",
       " 'should',\n",
       " 'not',\n",
       " 'be',\n",
       " 'a',\n",
       " 'redlink',\n",
       " 'given',\n",
       " 'the',\n",
       " 'deletion',\n",
       " 'reversion',\n",
       " 'what',\n",
       " 'should',\n",
       " 'be',\n",
       " 'the',\n",
       " 'outline',\n",
       " 'of',\n",
       " 'the',\n",
       " 'article',\n",
       " 'called',\n",
       " 'moneybomb',\n",
       " 'or',\n",
       " 'should',\n",
       " 'it',\n",
       " 'be',\n",
       " 'submitted',\n",
       " 'for',\n",
       " 'afd',\n",
       " 'again',\n",
       " 'in',\n",
       " 'due',\n",
       " 'time',\n",
       " 'if',\n",
       " 'the',\n",
       " 'latter',\n",
       " 'see',\n",
       " 'the',\n",
       " 'previous',\n",
       " 'version',\n",
       " 'of',\n",
       " 'however',\n",
       " 'this',\n",
       " 'version',\n",
       " 'will',\n",
       " 'require',\n",
       " 'a',\n",
       " 'detailed',\n",
       " 'answer',\n",
       " 'because',\n",
       " 'any',\n",
       " 'ambiguity',\n",
       " 'will',\n",
       " 'only',\n",
       " 'necessitate',\n",
       " 'clarifying',\n",
       " 'questions']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this to install stopwords as it might give LookUp error without it\n",
    "\n",
    "# import nltk   \n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stopwords with nltk.\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop = stopwords.words('english')\n",
    "\n",
    "df['text'] = df['text'].apply(lambda x: [item for item in x if item not in stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['december',\n",
       " 'utc',\n",
       " 'interested',\n",
       " 'arguing',\n",
       " 'policies',\n",
       " 'resolve',\n",
       " 'ongoing',\n",
       " 'content',\n",
       " 'dispute',\n",
       " 'also',\n",
       " 'see',\n",
       " 'wikipedia',\n",
       " 'wikiproject',\n",
       " 'united',\n",
       " 'states',\n",
       " 'presidential',\n",
       " 'elections',\n",
       " 'working',\n",
       " 'also',\n",
       " 'moneybomb',\n",
       " 'closer',\n",
       " 'self',\n",
       " 'reverted',\n",
       " 'two',\n",
       " 'different',\n",
       " 'requests',\n",
       " 'echoed',\n",
       " 'would',\n",
       " 'requested',\n",
       " 'rephrase',\n",
       " 'see',\n",
       " 'answer',\n",
       " 'building',\n",
       " 'agreement',\n",
       " 'moneybomb',\n",
       " 'redlink',\n",
       " 'given',\n",
       " 'deletion',\n",
       " 'reversion',\n",
       " 'outline',\n",
       " 'article',\n",
       " 'called',\n",
       " 'moneybomb',\n",
       " 'submitted',\n",
       " 'afd',\n",
       " 'due',\n",
       " 'time',\n",
       " 'latter',\n",
       " 'see',\n",
       " 'previous',\n",
       " 'version',\n",
       " 'however',\n",
       " 'version',\n",
       " 'require',\n",
       " 'detailed',\n",
       " 'answer',\n",
       " 'ambiguity',\n",
       " 'necessitate',\n",
       " 'clarifying',\n",
       " 'questions']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing between Lemmatization and stemming\n",
    "\n",
    "Stemming just removes or stems the last few characters of a word, often leading to incorrect meanings and spelling. Lemmatization considers the context and converts the word to its meaningful base form, which is called Lemma. Sometimes, the same word can have multiple different Lemmas.\n",
    "\n",
    "* If you lemmatize the word 'Caring', it would return 'Care'. If you stem, it would return 'Car' and this is erroneous.\n",
    "\n",
    "* If you lemmatize the word 'Stripes' in verb context, it would return 'Strip'. If you lemmatize it in noun context, it would return 'Stripe'. If you just stem it, it would just return 'Strip'.\n",
    "* You would get same results whether you lemmatize or stem words such as walking, running, swimming... to walk, run, swim etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this to install lemmetizer as it might give LookUp error without it\n",
    "\n",
    "# import nltk   \n",
    "# nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemma = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "# POS - Valid options are `\"n\"` for nouns,`\"v\"` for verbs, `\"a\"` for adjectives, `\"r\"` for adverbs and `\"s\"` for satellite adjectives.\n",
    "# Modularzation needed\n",
    "\n",
    "def clean_lemmatization_noun(token):\n",
    "    return [lemma.lemmatize(word = w, pos='n') for w in token]\n",
    "\n",
    "def clean_lemmatization_verb(token):\n",
    "    return [lemma.lemmatize(word = w, pos='v') for w in token]\n",
    "\n",
    "df['text'] = df['text'].apply(clean_lemmatization_noun)\n",
    "df['text'] = df['text'].apply(clean_lemmatization_verb)\n",
    "# df['text'] = df.apply(lambda x : clean_lemmatization(df['text'], 'n'), axis=1)\n",
    "# df['text'] = df.apply(lambda x : clean_lemmatization(df['text'], 'v'), axis=1)\n",
    "# df['newcolumn'] = df.apply(lambda x: fxy(x['A'], x['B']), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['december',\n",
       " 'utc',\n",
       " 'interest',\n",
       " 'argue',\n",
       " 'policy',\n",
       " 'resolve',\n",
       " 'ongoing',\n",
       " 'content',\n",
       " 'dispute',\n",
       " 'also',\n",
       " 'see',\n",
       " 'wikipedia',\n",
       " 'wikiproject',\n",
       " 'unite',\n",
       " 'state',\n",
       " 'presidential',\n",
       " 'election',\n",
       " 'work',\n",
       " 'also',\n",
       " 'moneybomb',\n",
       " 'closer',\n",
       " 'self',\n",
       " 'revert',\n",
       " 'two',\n",
       " 'different',\n",
       " 'request',\n",
       " 'echo',\n",
       " 'would',\n",
       " 'request',\n",
       " 'rephrase',\n",
       " 'see',\n",
       " 'answer',\n",
       " 'build',\n",
       " 'agreement',\n",
       " 'moneybomb',\n",
       " 'redlink',\n",
       " 'give',\n",
       " 'deletion',\n",
       " 'reversion',\n",
       " 'outline',\n",
       " 'article',\n",
       " 'call',\n",
       " 'moneybomb',\n",
       " 'submit',\n",
       " 'afd',\n",
       " 'due',\n",
       " 'time',\n",
       " 'latter',\n",
       " 'see',\n",
       " 'previous',\n",
       " 'version',\n",
       " 'however',\n",
       " 'version',\n",
       " 'require',\n",
       " 'detail',\n",
       " 'answer',\n",
       " 'ambiguity',\n",
       " 'necessitate',\n",
       " 'clarify',\n",
       " 'question']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>harsh</th>\n",
       "      <th>extremely_harsh</th>\n",
       "      <th>vulgar</th>\n",
       "      <th>threatening</th>\n",
       "      <th>disrespect</th>\n",
       "      <th>targeted_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a8be7c5d4527adbbf15f</td>\n",
       "      <td>[december, utc, interest, argue, policy, resol...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0b7ca73f388222aad64d</td>\n",
       "      <td>[add, three, miss, parameter, template, infobo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>db934381501872ba6f38</td>\n",
       "      <td>[sandbox, madre, sandbox]</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>228015c4a87c4b1f09a7</td>\n",
       "      <td>[good, sir, sir, obviously, comprehend, import...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b18f26cfa1408b52e949</td>\n",
       "      <td>[source, incase, forget, someone, else, want, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                               text  \\\n",
       "0  a8be7c5d4527adbbf15f  [december, utc, interest, argue, policy, resol...   \n",
       "1  0b7ca73f388222aad64d  [add, three, miss, parameter, template, infobo...   \n",
       "2  db934381501872ba6f38                          [sandbox, madre, sandbox]   \n",
       "3  228015c4a87c4b1f09a7  [good, sir, sir, obviously, comprehend, import...   \n",
       "4  b18f26cfa1408b52e949  [source, incase, forget, someone, else, want, ...   \n",
       "\n",
       "   harsh  extremely_harsh  vulgar  threatening  disrespect  targeted_hate  \n",
       "0      0                0       0            0           0              0  \n",
       "1      0                0       0            0           0              0  \n",
       "2      1                0       0            0           0              0  \n",
       "3      1                0       1            1           1              0  \n",
       "4      0                0       0            0           0              0  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text -> Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
